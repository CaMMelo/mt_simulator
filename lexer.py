from parser import Token as parser

def Token(tipo, valor):
    return(tipo, valor)

fsm = {
#          [ 1, 2, 3, 4, 5, 6, 7, 8, 9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35]
    '\t' : [ 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,33, 1, 1],
    '\n' : [ 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
    ' '  : [ 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,33, 1, 1],
    '!'  : [24,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,33,32,32],
    '*'  : [26,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,33,32,32],
    '-'  : [34,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,33,35,32],
    '0'  : [27,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,29,32,30,31,32,32,33,32,32],
    '1'  : [27,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,29,32,30,31,32,32,33,32,32],
    '2'  : [27,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,29,32,30,31,32,32,33,32,32],
    '3'  : [27,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,29,32,30,31,32,32,33,32,32],
    '4'  : [27,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,29,32,30,31,32,32,33,32,32],
    '5'  : [27,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,29,32,30,31,32,32,33,32,32],
    '6'  : [27,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,29,32,30,31,32,32,33,32,32],
    '7'  : [27,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,29,32,30,31,32,32,33,32,32],
    '8'  : [27,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,29,32,30,31,32,32,33,32,32],
    '9'  : [27,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,29,32,30,31,32,32,33,32,32],
    ';'  : [33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33,33],
    '_'  : [25,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,33,32,32],
    'a'  : [28,32,32,32,32,32,32,32,32,32,32,32,14,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,33,32,32],
    'b'  : [ 2,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,33,32,32],
    'c'  : [28,32,32, 5,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,33,32,32],
    'd'  : [ 7,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,33,32,32],
    'e'  : [ 9,32,32,32,32,32,32,32,32,32,32,32,32,32,16,32,18,32,32,32,32,23,32,32,32,32,32,32,32,32,32,32,33,32,32],
    'f'  : [10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,33,32,32],
    'i'  : [ 8,32,32,32,32,32,32,32,32,11,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,33,32,32],
    'l'  : [28, 3,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,33,32,32],
    'm'  : [28,32,32,32,32,32,32,32,32,32,12,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,33,32,32],
    'n'  : [28,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,22,32,32,32,32,32,32,32,32,32,32,32,33,32,32],
    'o'  : [28,32, 4,32, 6,32,32,32,32,32,32,32,32,32,32,32,32,32,20,32,32,32,32,32,32,32,32,32,32,32,32,32,33,32,32],
    'p'  : [13,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,33,32,32],
    'r'  : [17,32,32,32,32,32,32,32,32,32,32,32,32,15,32,32,32,32,32,21,32,32,32,32,32,32,32,32,32,32,32,32,33,32,32],
    't'  : [28,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,19,32,32,32,32,32,32,32,32,32,32,32,32,32,32,33,32,32],
    None : [28,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,33,32,32],
}

match = [
    parser.TK_NONE,
    parser.TK_ALPHA,
    parser.TK_PROCID,
    parser.TK_PROCID,
    parser.TK_PROCID,
    parser.TK_BLOCO,
    parser.TK_DIRECAO,
    parser.TK_DIRECAO,
    parser.TK_DIRECAO,
    parser.TK_ALPHA,
    parser.TK_PROCID,
    parser.TK_FIM,
    parser.TK_ALPHA,
    parser.TK_PROCID,
    parser.TK_PROCID,
    parser.TK_PARE,
    parser.TK_ALPHA,
    parser.TK_PROCID,
    parser.TK_PROCID,
    parser.TK_PROCID,
    parser.TK_PROCID,
    parser.TK_PROCID,
    parser.TK_RETORNE,
    parser.TK_PARADA,
    parser.TK_BRANCO,
    parser.TK_CORINGA,
    parser.TK_NUMERO,
    parser.TK_ALPHA,
    parser.TK_ESTADO,
    parser.TK_ESTADO,
    parser.TK_ESTADO,
    parser.TK_PROCID,
    parser.TK_NONE,
    parser.TK_ALPHA,
    parser.TK_DDASH,
]

def table(state, char):

    if char in fsm:
        state =  fsm[char][state-1]
    else:
        state = fsm[None][state-1]

    return state, match[state-1]

def tokenize(filename):
    ''' return a list of tokens from a stream of characters '''

    start       = 0
    lookahead   = 0
    state       = 1
    rule        = parser.TK_NONE

    tokens = []

    with open(filename, 'rb') as ifile:

        data = ifile.read()
        lexeme = ''

        while lookahead < len(data):

            c = chr(data[lookahead])
            state, _match = table(state, c)

            if (rule != parser.TK_NONE) and (_match == parser.TK_NONE):

                    tokens.append(Token(rule, lexeme))
                    lexeme = ''
                    rule = parser.TK_NONE

            rule = _match
            lookahead += 1

            print(c, state, rule)

            if rule != parser.TK_NONE:
                lexeme += str(c)

    return tokens
